{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从0开始构建一个简单的神经网络\n",
    "### 神经网络（Neural Network）\n",
    "\n",
    "\n",
    "### 关于这个Notebook\n",
    "\n",
    "在这个Notebook中，我们将学习如何使用Pytorch框架来构建简单的全连接神经网络，以及使用该网络对手写数字进行识别。\n",
    "首先，我们安装pytorch框架："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入我们需要的包\n",
    "Torchvision包括很多流行的数据集、模型架构和用于计算机视觉的常见图像转换模块，它是PyTorch项目的一部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mnist数据集介绍\n",
    "\n",
    "这个数据集包含 70,000 张手写体数字的灰度图(0=黑, 255=白)，这些图每一张的尺寸都是 28 像素乘以 28 像素，共 784 个像素，按照图像的标准来说，它们的尺寸非常小。这些图中的数字都是居中的，并且四边都留空了。这使得手写体识别可以比较方便地用标准的全连接网络来解决。\n",
    "下载数据集，并且构建dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = './data/'\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                torchvision.transforms.Normalize(mean = [0.5],std = [0.5])])\n",
    "trainData = torchvision.datasets.MNIST(path,train = True,transform = transform,download = True)\n",
    "testData = torchvision.datasets.MNIST(path,train = False,transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = trainData[0]\n",
    "print(image.shape) # 打印图像尺寸 (1, 28, 28)\n",
    "print(label) # 打印标签\n",
    "img = image.numpy().transpose(1, 2, 0)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练之前，我们需要指定训练设备，构建dataloader：\n",
    "\n",
    "1.指定训练设备,device：将张量部署在指定的运算设备上进行计算\n",
    "\n",
    "2.构建dataloader，*DataLoader*是 PyTorch 提供的一个数据加载器，用于对数据进行批量加载和处理。它在训练神经网络时起到了重要的作用。DataLoader 可以自动完成数据的批量加载、随机洗牌（shuffle）、并发预取等操作，从而提高模型训练的效率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 256 \n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(dataset = trainData,batch_size = BATCH_SIZE,shuffle = True)#shuffle = True 意味着会对数据集打乱\n",
    "testloader = torch.utils.data.DataLoader(dataset = testData,batch_size = BATCH_SIZE)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搭建神经网络\n",
    "首先搭建最基本的全连接3层网络，输入层 784个input，第一个隐藏层包含64个神经元，relu函数作为激活函数，第二个隐藏层包含64个神经元，relu函数作为激活函数，输出层10个神经元，softmax作为激活函数(但是请注意，这一层激活函数是包含在了损失函数里面，网络中没有这一层)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "\n",
    "        #两个全连接的隐藏层，一个输出层\n",
    "        #因为图片是28*28的，需要全部展开，最终我们要输出数字，一共10个数字。\n",
    "        #10个数字实际上是10个类别，输出是概率分布，最后选取概率最大的作为预测值输出\n",
    "        hidden_1 = 64\n",
    "        hidden_2 = 64\n",
    "        self.fc1 = nn.Linear(28 * 28,hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1,hidden_2)\n",
    "        self.fc3 = nn.Linear(hidden_2,10)\n",
    "        #self.dropout = nn.Dropout(0.2),防止过拟合\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x.view(-1,28 * 28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        #x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "model = Net().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = torch.rand(28,28)\n",
    "print(t2.dtype)\n",
    "t = t2.to(device)\n",
    "model.eval()\n",
    "y = model(t)\n",
    "print(y)\n",
    "y = torch.nn.functional.softmax(y,dim=1)    \n",
    "print(y)\n",
    "y.argmax()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in model.parameters():\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义损失函数：这里采用交叉熵损失函数\n",
    "#优化器：这里选用RMSprop 也可以采用（SGD、Adam）\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(params = model.parameters(),lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1  \tTraining Loss: 1.958830\n",
      "0.01\n",
      "Epoch:  1  \tTest Accuracy: 0.677600\n",
      "Epoch:  2  \tTraining Loss: 0.934634\n",
      "0.01\n",
      "Epoch:  2  \tTest Accuracy: 0.836100\n"
     ]
    }
   ],
   "source": [
    "#训练\n",
    "n_epochs = 2\n",
    "\n",
    "for epoch in range(0,n_epochs):\n",
    "    train_loss = 0.0\n",
    "    train_loss_ = 0.0\n",
    "  \n",
    "    model.train() # 开启训练模式，参数可更新\n",
    "    for data,target in trainloader:\n",
    "        data,target = data.to(device), target.to(device)\n",
    "       # print(data.size(0))\n",
    "        optimizer.zero_grad() # 每一步都需要将梯度归零，防止累计\n",
    "        output = model(data)#得到预测值\n",
    "\n",
    "        loss = criterion(output,target)\n",
    "        loss.backward()  #反向传播，计算梯度\n",
    "\n",
    "        optimizer.step() #使用梯度对网络参数进行更新\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "    train_loss = train_loss / len(trainloader.dataset)\n",
    "    print('Epoch:  {}  \\tTraining Loss: {:.6f}'.format(epoch + 1,train_loss))\n",
    "  \n",
    "    model.eval() #开启评估模式，参数不可更新,不进行反省传播，当网络中存在dropout和bn的时候，一定要加上。\n",
    "    print(optimizer.param_groups[0]['lr'])\n",
    "    correct = 0\n",
    "    for data, target in testloader:\n",
    "        data,target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        out = torch.argmax(output, dim=1)\n",
    "        correct += torch.sum(out == target)\n",
    "  \n",
    "    testAccuracy = correct / len(testloader.dataset)\n",
    "    print('Epoch:  {}  \\tTest Accuracy: {:.6f}'.format(epoch + 1,testAccuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过分析每轮训练出来的loss及accuracy变化，分析模型误差的原因，是过拟合还是欠拟合，从而采取相应的措施进行优化，例如：过拟合可以增加dropout比例、增加数据量或者正则化等方法。欠拟合，可以增加模型复杂度、优化学习率，增加特征数量等。\n",
    "### 模型的保存及测试\n",
    "可以使用model.load(filepath)将模型和权重保存在一个文件中，主要有两种方法：\n",
    "\n",
    "1.只保存参数；\n",
    "2.保存整个模型 (结构+参数)。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存整个model的状态\n",
    "torch.save(model, 'model0812.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推理过程\n",
    "可以使用model.load(filepath)来重新加载你的模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torchvision\n",
    "model_predict = torch.load(\"model0812.pth\").to(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "tensor([[-6.8162, -0.4700,  8.8663, -3.2106, -1.7154, -0.4724,  6.2760, -4.5514,\n",
      "         -4.4659, -3.1768]], grad_fn=<AddmmBackward0>)\n",
      "tensor([8.8663], grad_fn=<MaxBackward0>) tensor([2])\n",
      "Predicted class:  2\n"
     ]
    }
   ],
   "source": [
    "data = cv2.imread('./testpic/2_2.png')\n",
    "data = cv2.cvtColor(data,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "data_normal = cv2.resize(data,(28,28),cv2.INTER_LINEAR)\n",
    "\n",
    "input_data = transform(data_normal)\n",
    "print(input_data.shape)\n",
    "output = model_predict(input_data)\n",
    "\n",
    "print(output)\n",
    "\n",
    "_, predicted = torch.max(output, 1)\n",
    "print(_,predicted)\n",
    "print('Predicted class: ', predicted.item())#item 单个元素 转为标量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 了解损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy\n",
    "x = sympy.symbols('x')\n",
    "sigma = 1/(1 + sympy.exp(-x))\n",
    "tanh = (sympy.exp(x) - sympy.exp(-x)) / (sympy.exp(x) + sympy.exp(-x))\n",
    "dsigma = sigma.diff(x)\n",
    "dtanh = tanh.diff(x)\n",
    "sympy.plot(dtanh)\n",
    "sympy.plot(dsigma)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
